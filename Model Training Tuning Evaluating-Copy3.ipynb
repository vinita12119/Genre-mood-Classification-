{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Reshape\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.recurrent import GRU\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.417  0.646  0.128  0.0192]\n",
      " [0.417  0.646  0.128  0.0192]\n",
      " [0.417  0.646  0.128  0.0192]\n",
      " ...\n",
      " [0.206  0.658  0.033  0.472 ]\n",
      " [0.206  0.658  0.033  0.472 ]\n",
      " [0.206  0.658  0.033  0.472 ]]\n",
      "(3750,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinitachaudhary/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xDataRaw = pd.read_csv('xDataMelSmaller.csv' )\n",
    "genreDataRaw = pd.read_csv( 'genreDataSmaller.csv ))\n",
    "featDataRaw = pickle.load(open( 'featDataSmaller.csv\" ))\n",
    "\n",
    "featDataRaw = featDataRaw[:,0:4]\n",
    "\n",
    "xData = np.asarray(xDataRaw) / 255.0\n",
    "genreData = np.array(genreDataRaw)\n",
    "featData= np.array(featDataRaw)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(genreData)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "genres = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(featData)\n",
    "print(genreDataRaw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "(3750, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(genres)\n",
    "print(xData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertValenceToLabel(data):\n",
    "    if data<=.3:\n",
    "        return 'Sad'\n",
    "    elif (data>.3 and data<=.6):\n",
    "        return 'moodNeutral'\n",
    "    else:\n",
    "        return 'Happy'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "danceability = featData[:,0]\n",
    "energy = featData[:,1]\n",
    "valence = featData[:,2]\n",
    "accousticness = featData[:,3]\n",
    "ValenceData = []\n",
    "for data in valence:\n",
    "    ValenceData.extend([ConvertValenceToLabel(data) ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinitachaudhary/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "label_encoder_valence = LabelEncoder()\n",
    "onehot_encoder_valence = OneHotEncoder(sparse=False)\n",
    "integer_encoded_Valence = label_encoder_valence.fit_transform(ValenceData)\n",
    "integer_encoded_Valence = integer_encoded_Valence.reshape(len(integer_encoded_Valence), 1)\n",
    "valenceOneHot = onehot_encoder.fit_transform(integer_encoded_Valence)\n",
    "print(valenceOneHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188,)\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_portion = .05\n",
    "dev_portion = .05\n",
    "test_size = round(len(xData)*test_portion)\n",
    "dev_size = round(len(xData)*dev_portion)\n",
    "randTotal= np.random.randint(1,len(xData),test_size+dev_size)\n",
    "randTest = randTotal[0:test_size]\n",
    "randDev = randTotal[test_size:]\n",
    "origIndices= list(range(0, len(xData)))\n",
    "trainIndices = [x for x in origIndices if x not in randTotal]\n",
    "\n",
    "print(randDev.shape)\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devX = xData[randDev]\n",
    "devGenres =genres[randDev]\n",
    "devValence =valenceOneHot[randDev]\n",
    "\n",
    "trainX = xData[trainIndices]\n",
    "trainGenres =genres[trainIndices]\n",
    "trainValence =valenceOneHot[trainIndices]\n",
    "\n",
    "testX = xData[randTest]\n",
    "testGenres = genres[randTest]\n",
    "testValence = valenceOneHot[randTest]\n",
    "\n",
    "\n",
    "testX=testX.reshape(testX.shape[0],testX.shape[1],testX.shape[2],1)\n",
    "trainX=trainX.reshape(trainX.shape[0],trainX.shape[1],trainX.shape[2],1)\n",
    "devX = devX.reshape(devX.shape[0],devX.shape[1],devX.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3397, 128, 128, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        self.json_path = json_path\n",
    "\n",
    "    def save(self, json_path):\n",
    "        \"\"\"Saves parameters to json file\"\"\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.__dict__, f, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        \"\"\"Loads parameters from json file\"\"\"\n",
    "        with open(json_path) as f:\n",
    "            params = json.load(f)\n",
    "            self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_axis = 3\n",
    "freq_axis = 1\n",
    "time_axis = 2\n",
    "\n",
    "\n",
    "def lvl1(width, height, depth,classesGenres, param,activate=\"softmax\"):\n",
    "\n",
    "    regparam = param.regparam\n",
    "    dropout = param.dropout\n",
    "    \n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        print (\"th\")\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        print(\"not th\")\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "\n",
    "    if K.image_data_format() =='channels_first':\n",
    "        input_shape=(1,width,height)\n",
    "    else:\n",
    "        input_shape=(width,height,1)\n",
    "    def convRelu(i, batchNormalization=False):\n",
    "        nIn = nc if i == 0 else nm[i - 1]\n",
    "        nOut = nm[i]\n",
    "        model.add_module('conv{0}'.format(i),nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "        if batchNormalization:\n",
    "            model.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "        if leakyRelu:\n",
    "            model.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "        else:\n",
    "            model.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "     # Input block\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D(padding=(0, 40),input_shape= input_shape))\n",
    "    model.add(BatchNormalization(axis=freq_axis, name='bn_0_freq',input_shape= input_shape))\n",
    "    # Conv -1 \n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=3, kernel_initializer='glorot_normal',padding='same', name='Conv1', input_shape= input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis, name='bn1'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),padding='same', name='pool1'))\n",
    "    model.add(Dropout(0.1, name='dropout1'))\n",
    "    \n",
    "     # Conv -2 \n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=3, kernel_initializer='glorot_normal',padding='same', name='Conv2', input_shape= input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis, name='bn2'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3),strides=(3,3),padding='same', name='pool2'))\n",
    "    model.add(Dropout(0.1, name='dropout2'))\n",
    "    \n",
    "     # Conv -3 \n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=3, kernel_initializer='glorot_normal',padding='same', name='Conv3', input_shape= input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis, name='bn3'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4),strides=(4,4),padding='same', name='pool3'))\n",
    "    model.add(Dropout(0.1, name='dropout3'))\n",
    "    \n",
    "     # Conv -4 \n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=3, kernel_initializer='glorot_normal',padding='same', name='Conv4', input_shape= input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis, name='bn4'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4,4),strides=(4,4), padding='same'))\n",
    "    model.add(Dropout(0.1, name='dropout4'))\n",
    "   #model.add(Conv2D(filters=512, kernel_size=2, strides=2, kernel_initializer='glorot_normal',padding='same', name='Conv5', input_shape= input_shape))\n",
    "\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "         model.add(Permute((3, 1, 2)))\n",
    "    model.add(Reshape((1, 128)))\n",
    "    #convRelu(6,True)\n",
    "    #model.add(Flatten())\n",
    "    \n",
    "  #  GRU block 1, 2, output\n",
    "    model.add(GRU(32,input_shape=(15,512),return_sequences=True, name='gru1'))\n",
    "    model.add(GRU(32, return_sequences=False, name='gru2'))\n",
    "    model.add(Dropout(0.3,name='dropout5'))\n",
    "   \n",
    "    model.add(Dense(3, activation='sigmoid', name='output'))\n",
    "    #model.add(Flatten(name='Flatten'))\n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy' 'Sad' 'moodNeutral']\n",
      "['Electronic' 'Folk' 'Instrumental' 'Pop' 'Rock']\n"
     ]
    }
   ],
   "source": [
    "'''These are the params I ended up using'''\n",
    "param = Params('params.json')\n",
    "param.dict['LR']=.001\n",
    "param.dict['BatchSize']=150\n",
    "param.dict['regparam']=.001\n",
    "param.dict['dropout'] = .5\n",
    "param.dict['FCLayers']=1\n",
    "EPOCHS = 75\n",
    "print(label_encoder_valence.classes_)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not th\n"
     ]
    }
   ],
   "source": [
    "IMAGE_DIMS = trainX[0].shape\n",
    "H= lvl1(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],depth=IMAGE_DIMS[2], classesGenres = len(label_encoder_valence.classes_),param=param, activate=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=param.LR)\n",
    "H.compile(loss= \"categorical_crossentropy\",loss_weights={'output': 1}, optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "#filepath=\"weights.best.hdf5\"\n",
    "filepath=\"weightsCheckpoint.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', \n",
    "                             verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3397 samples, validate on 188 samples\n",
      "Epoch 1/75\n",
      "3397/3397 [==============================] - 45s 13ms/step - loss: 1.0048 - acc: 0.4854 - val_loss: 0.9430 - val_acc: 0.5319\n",
      "Epoch 2/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.9365 - acc: 0.5378 - val_loss: 0.9559 - val_acc: 0.5638\n",
      "Epoch 3/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.8851 - acc: 0.5687 - val_loss: 0.8897 - val_acc: 0.5691\n",
      "Epoch 4/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.8627 - acc: 0.5879 - val_loss: 0.8357 - val_acc: 0.6064\n",
      "Epoch 5/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.8247 - acc: 0.6155 - val_loss: 0.8961 - val_acc: 0.5426\n",
      "Epoch 6/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.8012 - acc: 0.6420 - val_loss: 0.9155 - val_acc: 0.5691\n",
      "Epoch 7/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.7720 - acc: 0.6582 - val_loss: 0.8311 - val_acc: 0.6117\n",
      "Epoch 8/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.7346 - acc: 0.6774 - val_loss: 0.8550 - val_acc: 0.6330\n",
      "Epoch 9/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.7226 - acc: 0.6824 - val_loss: 1.0897 - val_acc: 0.5372\n",
      "Epoch 10/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.6708 - acc: 0.7053 - val_loss: 0.7185 - val_acc: 0.6915\n",
      "Epoch 11/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.6529 - acc: 0.7118 - val_loss: 0.7183 - val_acc: 0.6489\n",
      "Epoch 12/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.6219 - acc: 0.7333 - val_loss: 0.8716 - val_acc: 0.5213\n",
      "Epoch 13/75\n",
      "3397/3397 [==============================] - 46s 14ms/step - loss: 0.6165 - acc: 0.7306 - val_loss: 0.7533 - val_acc: 0.6702\n",
      "Epoch 14/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.5671 - acc: 0.7583 - val_loss: 0.7393 - val_acc: 0.6649\n",
      "Epoch 15/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.5625 - acc: 0.7745 - val_loss: 0.8304 - val_acc: 0.5957\n",
      "Epoch 16/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.5372 - acc: 0.7769 - val_loss: 0.9846 - val_acc: 0.6064\n",
      "Epoch 17/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.4921 - acc: 0.7998 - val_loss: 0.7449 - val_acc: 0.6702\n",
      "Epoch 18/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.4645 - acc: 0.8078 - val_loss: 1.2115 - val_acc: 0.5479\n",
      "Epoch 19/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.4386 - acc: 0.8219 - val_loss: 0.8350 - val_acc: 0.6649\n",
      "Epoch 20/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.4288 - acc: 0.8310 - val_loss: 0.7687 - val_acc: 0.6968\n",
      "Epoch 21/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.4097 - acc: 0.8357 - val_loss: 0.8566 - val_acc: 0.6489\n",
      "Epoch 22/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.3564 - acc: 0.8608 - val_loss: 0.8041 - val_acc: 0.6862\n",
      "Epoch 23/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.3438 - acc: 0.8675 - val_loss: 0.6188 - val_acc: 0.7660\n",
      "Epoch 24/75\n",
      "3397/3397 [==============================] - 45s 13ms/step - loss: 0.3369 - acc: 0.8714 - val_loss: 0.8457 - val_acc: 0.6702\n",
      "Epoch 25/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.3313 - acc: 0.8731 - val_loss: 1.0102 - val_acc: 0.6170\n",
      "Epoch 26/75\n",
      "3397/3397 [==============================] - 40s 12ms/step - loss: 0.3175 - acc: 0.8817 - val_loss: 0.9280 - val_acc: 0.7287\n",
      "Epoch 27/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.3142 - acc: 0.8817 - val_loss: 0.8053 - val_acc: 0.7394\n",
      "Epoch 28/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.2528 - acc: 0.9055 - val_loss: 1.1638 - val_acc: 0.5851\n",
      "Epoch 29/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.2423 - acc: 0.9152 - val_loss: 0.9007 - val_acc: 0.7340\n",
      "Epoch 30/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.2512 - acc: 0.9108 - val_loss: 1.0338 - val_acc: 0.6436\n",
      "Epoch 31/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.2232 - acc: 0.9211 - val_loss: 0.7836 - val_acc: 0.7074\n",
      "Epoch 32/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.2023 - acc: 0.9267 - val_loss: 0.6345 - val_acc: 0.7447\n",
      "Epoch 33/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.2351 - acc: 0.9211 - val_loss: 0.8284 - val_acc: 0.7287\n",
      "Epoch 34/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.2301 - acc: 0.9155 - val_loss: 0.7571 - val_acc: 0.7447\n",
      "Epoch 35/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.2195 - acc: 0.9190 - val_loss: 0.6580 - val_acc: 0.7819\n",
      "Epoch 36/75\n",
      "3397/3397 [==============================] - 42s 13ms/step - loss: 0.2602 - acc: 0.9055 - val_loss: 1.0110 - val_acc: 0.7074\n",
      "Epoch 37/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1751 - acc: 0.9397 - val_loss: 0.8625 - val_acc: 0.7287\n",
      "Epoch 38/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.6017 - val_acc: 0.7926\n",
      "Epoch 39/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1827 - acc: 0.9355 - val_loss: 0.8518 - val_acc: 0.7128\n",
      "Epoch 40/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.2030 - acc: 0.9255 - val_loss: 0.6009 - val_acc: 0.7819\n",
      "Epoch 41/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1809 - acc: 0.9388 - val_loss: 0.7345 - val_acc: 0.7660\n",
      "Epoch 42/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.1705 - acc: 0.9435 - val_loss: 0.8620 - val_acc: 0.7553\n",
      "Epoch 43/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1461 - acc: 0.9497 - val_loss: 0.6939 - val_acc: 0.7819\n",
      "Epoch 44/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1219 - acc: 0.9591 - val_loss: 1.0880 - val_acc: 0.6968\n",
      "Epoch 45/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1421 - acc: 0.9505 - val_loss: 0.9726 - val_acc: 0.7181\n",
      "Epoch 46/75\n",
      "3397/3397 [==============================] - 42s 13ms/step - loss: 0.1524 - acc: 0.9494 - val_loss: 0.8172 - val_acc: 0.7553\n",
      "Epoch 47/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1148 - acc: 0.9594 - val_loss: 0.7595 - val_acc: 0.7819\n",
      "Epoch 48/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1299 - acc: 0.9570 - val_loss: 0.6819 - val_acc: 0.7872\n",
      "Epoch 49/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1140 - acc: 0.9620 - val_loss: 0.7475 - val_acc: 0.7606\n",
      "Epoch 50/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.0877 - acc: 0.9682 - val_loss: 0.8018 - val_acc: 0.7606\n",
      "Epoch 51/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1058 - acc: 0.9641 - val_loss: 1.0092 - val_acc: 0.7447\n",
      "Epoch 52/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1890 - acc: 0.9338 - val_loss: 0.7332 - val_acc: 0.7713\n",
      "Epoch 53/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1211 - acc: 0.9550 - val_loss: 0.7783 - val_acc: 0.7287\n",
      "Epoch 54/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.1157 - acc: 0.9623 - val_loss: 1.0117 - val_acc: 0.7021\n",
      "Epoch 55/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1065 - acc: 0.9653 - val_loss: 0.7781 - val_acc: 0.7500\n",
      "Epoch 56/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.0958 - acc: 0.9694 - val_loss: 0.7238 - val_acc: 0.7713\n",
      "Epoch 57/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1045 - acc: 0.9638 - val_loss: 0.8559 - val_acc: 0.7606\n",
      "Epoch 58/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.0856 - acc: 0.9712 - val_loss: 1.0695 - val_acc: 0.7234\n",
      "Epoch 59/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0735 - acc: 0.9759 - val_loss: 0.7582 - val_acc: 0.7979\n",
      "Epoch 60/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0914 - acc: 0.9723 - val_loss: 0.7360 - val_acc: 0.8404\n",
      "Epoch 61/75\n",
      "3397/3397 [==============================] - 44s 13ms/step - loss: 0.0862 - acc: 0.9700 - val_loss: 0.7841 - val_acc: 0.7340\n",
      "Epoch 62/75\n",
      "3397/3397 [==============================] - 44s 13ms/step - loss: 0.0952 - acc: 0.9635 - val_loss: 0.8915 - val_acc: 0.7926\n",
      "Epoch 63/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0840 - acc: 0.9747 - val_loss: 0.8852 - val_acc: 0.7234\n",
      "Epoch 64/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1033 - acc: 0.9664 - val_loss: 1.3521 - val_acc: 0.6543\n",
      "Epoch 65/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.1669 - acc: 0.9450 - val_loss: 0.8720 - val_acc: 0.7394\n",
      "Epoch 66/75\n",
      "3397/3397 [==============================] - 44s 13ms/step - loss: 0.1133 - acc: 0.9614 - val_loss: 0.8604 - val_acc: 0.7553\n",
      "Epoch 67/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0902 - acc: 0.9685 - val_loss: 0.8349 - val_acc: 0.7766\n",
      "Epoch 68/75\n",
      "3397/3397 [==============================] - 44s 13ms/step - loss: 0.0759 - acc: 0.9776 - val_loss: 0.8956 - val_acc: 0.7340\n",
      "Epoch 69/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0734 - acc: 0.9735 - val_loss: 0.7008 - val_acc: 0.7979\n",
      "Epoch 70/75\n",
      "3397/3397 [==============================] - 43s 13ms/step - loss: 0.0771 - acc: 0.9735 - val_loss: 0.8911 - val_acc: 0.7766\n",
      "Epoch 71/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.0699 - acc: 0.9762 - val_loss: 1.1005 - val_acc: 0.7340\n",
      "Epoch 72/75\n",
      "3397/3397 [==============================] - 41s 12ms/step - loss: 0.1148 - acc: 0.9623 - val_loss: 0.8933 - val_acc: 0.7660\n",
      "Epoch 73/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.0682 - acc: 0.9809 - val_loss: 0.8666 - val_acc: 0.7872\n",
      "Epoch 74/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.0959 - acc: 0.9685 - val_loss: 0.8525 - val_acc: 0.7819\n",
      "Epoch 75/75\n",
      "3397/3397 [==============================] - 42s 12ms/step - loss: 0.1000 - acc: 0.9682 - val_loss: 1.0314 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_val_gru = H.fit(trainX,trainValence,validation_data=(devX,{\"output\": devValence}),epochs=EPOCHS,callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "callbacks=callbacks_list,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import numpy as np\n",
    "matplotlib.use('agg')\n",
    "class TrainingPlot(keras.callbacks.Callback):\n",
    "\n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.acc = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "\n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        # Append the logs, losses and accuracies to the lists\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pydotplus\n",
    "import pydot as pyd\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "plot_model(H, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_gru_samp_json = H.to_json()\n",
    "with open(\"model_gru_samp_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_gru_samp_json)\n",
    "# serialize weights to HDF5\n",
    "H.save_weights(\"model_gru_samp_json.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 6ms/step\n",
      "acc: 74.47%\n"
     ]
    }
   ],
   "source": [
    "model_test_val_gru = H.evaluate(testX, [testValence], verbose=1)\n",
    "H.metrics_names\n",
    "#H.scores\n",
    "print(\"%s: %.2f%%\" % (H.metrics_names[1], model_test_val_gru[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 5ms/step\n",
      "acc: 81.38%\n"
     ]
    }
   ],
   "source": [
    "model_test_gru = H.evaluate(testX, [testGenres], verbose=1)\n",
    "H.metrics_names\n",
    "#H.scores\n",
    "print(\"%s: %.2f%%\" % (H.metrics_names[1], model_test_gru[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = H.fit(trainX,trainGenres,validation_data=(devX,{\"output\": devGenres}),epochs=EPOCHS,callbacks=callbacks_list, verbose=1)\n",
    "#callbacks=callbacks_list,\n",
    "# save the model to disk\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
